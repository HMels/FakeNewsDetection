{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f128bf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset liar (C:/Users/Mels/.cache/huggingface/datasets/liar/default/1.0.0/479463e757b7991eed50ffa7504d7788d6218631a484442e2098dabbf3b44514)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53cf18e55094ae9aca7388b5fe0b171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nUser: Mels Habold\\nData downloaded from https://huggingface.co/datasets/liar\\nBased on research from https://arxiv.org/abs/1705.00648\\nAnd an article posted in https://www.analyticssteps.com/blogs/detection-fake-and-false-news-text-analysis-approaches-and-cnn-deep-learning-model \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"liar\")   \n",
    "\n",
    "'''\n",
    "User: Mels Habold\n",
    "Data downloaded from https://huggingface.co/datasets/liar\n",
    "Based on research from https://arxiv.org/abs/1705.00648\n",
    "And an article posted in https://www.analyticssteps.com/blogs/detection-fake-and-false-news-text-analysis-approaches-and-cnn-deep-learning-model \n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4e4c1",
   "metadata": {},
   "source": [
    "## The LIAR Dataset\n",
    "\n",
    "We consider six fine-grained labels for the truthfulness ratings: \n",
    "pants-fire, false, barelytrue, half-true, mostly-true, and true.\n",
    "\n",
    "It consists of three sets: The training set is used to fit the model's parameters, while the validation set is used to tune the model's hyperparameters (e.g., learning rate, number of layers, etc.) and to monitor the model's performance during training. The test set is used to evaluate the final performance of the model after it has been trained and fine-tuned using the training and validation sets.\n",
    "\n",
    "The next code allows us to split the data into 3 types:\n",
    "False, in between, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8efa7220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "if True:\n",
    "    train_label = np.array(dataset[\"train\"]['label'])\n",
    "    test_label = np.array(dataset[\"test\"]['label'])\n",
    "    validation_label = np.array(dataset[\"validation\"]['label'])\n",
    "    \n",
    "    train_label = np.where(train_label==1, 0, train_label)\n",
    "    train_label = np.where(train_label==2, 1, train_label)\n",
    "    train_label = np.where(train_label==3, 1, train_label)\n",
    "    train_label = np.where(train_label==4, 2, train_label)\n",
    "    train_label = np.where(train_label==5, 2, train_label)\n",
    "    \n",
    "    test_label = np.where(test_label==1, 0, test_label)\n",
    "    test_label = np.where(test_label==2, 1, test_label)\n",
    "    test_label = np.where(test_label==3, 1, test_label)\n",
    "    test_label = np.where(test_label==4, 2, test_label)\n",
    "    test_label = np.where(test_label==5, 2, test_label)\n",
    "    \n",
    "    validation_label = np.where(validation_label==1, 0, validation_label)\n",
    "    validation_label = np.where(validation_label==2, 1, validation_label)\n",
    "    validation_label = np.where(validation_label==3, 1, validation_label)\n",
    "    validation_label = np.where(validation_label==4, 2, validation_label)\n",
    "    validation_label = np.where(validation_label==5, 2, validation_label)\n",
    "\n",
    "else:\n",
    "    train_label = dataset[\"train\"]['label']\n",
    "    test_label = dataset[\"test\"]['label']\n",
    "    validation_label = dataset[\"validation\"]['label']\n",
    "    \n",
    "print(np.unique(train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7296992",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "The first part of the code imports the necessary libraries and defines the hyperparameters. Then, it loads the training data from a dataset object and tokenizes the text using the Tokenizer class from Keras. The Tokenizer class is used to convert text to sequences of integers. The num_words parameter specifies the maximum number of words to keep, based on word frequency. The next step is to pad the sequences so that all input data have the same length using the pad_sequences function from Keras.\n",
    "\n",
    "After the preprocessing steps, the code loads a pre-trained Word2Vec model using the Gensim library. Then, it creates an embedding matrix using the Word2Vec model to map each word in the input sequences to a corresponding vector in the embedding space. The embedding matrix is used to initialize the embedding layer in the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9fd8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#define the hyperparameters\n",
    "max_words = 100                 # max number of words in a statement\n",
    "embedding_dim = 50              # dimension of the word vector\n",
    "output_dim = tf.unique(train_label)[0].shape[0]  # number of output labels\n",
    "\n",
    "#tokenize the statements\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(dataset[\"train\"]['statement'])\n",
    "sequences_statement = tokenizer.texts_to_sequences(dataset[\"train\"]['statement'])\n",
    "\n",
    "#tokenize the statements\n",
    "tokenizer.fit_on_texts(dataset[\"train\"]['subject'])\n",
    "sequences_subject = tokenizer.texts_to_sequences(dataset[\"train\"]['subject'])\n",
    "\n",
    "#pad the sequences\n",
    "x_data_subject = pad_sequences(sequences_statement, maxlen=max_words)\n",
    "x_data_statement = pad_sequences(sequences_subject, maxlen=max_words)\n",
    "y_data = to_categorical(train_label, num_classes=output_dim)\n",
    "\n",
    "x_data = [x_data_subject, x_data_statement]\n",
    "sequences_combined = sequences_statement + sequences_subject\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "#train Word2Vec model\n",
    "sentences = dataset[\"train\"]['statement'] + dataset[\"train\"]['subject']\n",
    "sentences = [sentence.split() for sentence in sentences]\n",
    "w2v_model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_words:\n",
    "        try:\n",
    "            embedding_vector = w2v_model.wv[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "# Create the embedding layer\n",
    "embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim,\n",
    "                            weights=[embedding_matrix], input_length=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab4cfe7",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    "\n",
    "1. Embedding layer: This layer maps the integer-encoded vocabulary to dense vectors of fixed size, in this case, an embedding vector of dimension embedding_dim for each word. The input_dim parameter specifies the size of the vocabulary, which is set to max_words in this code. The output_dim parameter specifies the size of the embedding vector.\n",
    "\n",
    "2. Bidirectional layer that wraps around the LSTM layer. The Bidirectional layer takes the LSTM layer as input and creates two copies of it, one for processing the text forward and the other for processing it backward. The outputs from these two layers are then concatenated and passed to the next layer.\n",
    "\n",
    "3. Convolutional layer: This layer applies num_filters filters of size kernel_size to the input sequence. The filters slide over the input sequence, producing a feature map of convolved features. The activation parameter is set to ReLU, which means that the output of the layer will be the rectified linear activation function of the convolved features.\n",
    "\n",
    "4. Max pooling layer: This layer downsamples the convolved features by taking the maximum value of each non-overlapping pool_size-sized segment of the feature map.\n",
    "\n",
    "5. Flatten layer: This layer flattens the output of the previous layer to a 1D array.\n",
    "\n",
    "6. Dense layer: This layer is a fully connected layer with hidden_dim units. The activation function used is ReLU, which applies the rectified linear activation function to the output.\n",
    "\n",
    "7. Dropout layers randomly drop out some of the neurons during training, which can help prevent overfitting. This can be added using the Dropout layer in Keras.\n",
    "\n",
    "8. Output layer: This layer is a fully connected layer with output_dim units, which corresponds to the number of output classes. The activation function used is softmax, which produces a probability distribution over the classes.\n",
    "\n",
    "In summary, the CNN takes as input a sequence of integers representing the words in a text, and passes it through an embedding layer to obtain dense vectors. Then, the convolutional layer applies a set of filters to the sequence, producing a feature map that is downsampled by the max pooling layer. The flatten layer converts the output of the max pooling layer into a 1D array, which is passed through a dense layer and finally an output layer to produce a probability distribution over the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f705868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 100, 50)      5000        ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 100, 100)     0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 98, 128)      38528       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 49, 128)      0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 6272)         0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           313650      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            153         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 357,331\n",
      "Trainable params: 357,331\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define the hyperparameters\n",
    "num_filters = 128            # number of convolutional filters\n",
    "kernel_size = 3              # size of convolutional kernel\n",
    "pool_size = 2                # size of pooling window\n",
    "hidden_dim = 50              # dimension of fully connected layers\n",
    "\n",
    "# Define the two inputs\n",
    "input_subject = Input(shape=(max_words,), dtype='int32')\n",
    "input_statement = Input(shape=(max_words,), dtype='int32')\n",
    "\n",
    "# Define the embedding layer\n",
    "#embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_words)\n",
    "\n",
    "# Apply the embedding layer to both inputs\n",
    "embedded_subject = embedding_layer(input_subject)\n",
    "embedded_statement = embedding_layer(input_statement)\n",
    "\n",
    "# Merge the two embedded inputs using the Concatenate layer\n",
    "merged = Concatenate(axis=-1)([embedded_subject, embedded_statement])\n",
    "\n",
    "# Define the model\n",
    "conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_size, activation='softmax')(merged)\n",
    "pool_layer = MaxPooling1D(pool_size=pool_size)(conv_layer)\n",
    "flatten_layer = Flatten()(pool_layer)\n",
    "hidden_layer = Dense(hidden_dim, activation='relu')(flatten_layer)\n",
    "output = Dense(output_dim, activation='softmax')(hidden_layer)\n",
    "\n",
    "model = Model(inputs=[input_subject, input_statement], outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e02889",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The second part of the code trains the CNN using the fit function in Keras. The batch_size parameter specifies the number of samples processed in one batch, and the epochs parameter specifies the number of training epochs. The compile function is used to configure the learning process, including the optimizer, loss function, and evaluation metric. The adam optimizer is used, and the categorical_crossentropy loss function is used since this is a multi-class classification problem. The evaluation metric is accuracy.\n",
    "\n",
    "After training, the model is evaluated using the evaluate function in Keras, and the test loss and accuracy are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92bd85d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "289/289 [==============================] - 7s 22ms/step - loss: 1.0799 - accuracy: 0.3966 - val_loss: 1.0773 - val_accuracy: 0.4119\n",
      "Epoch 2/10\n",
      "289/289 [==============================] - 6s 22ms/step - loss: 1.0763 - accuracy: 0.3987 - val_loss: 1.0749 - val_accuracy: 0.4119\n",
      "Epoch 3/10\n",
      "289/289 [==============================] - 6s 22ms/step - loss: 1.0733 - accuracy: 0.4039 - val_loss: 1.0697 - val_accuracy: 0.4148\n",
      "Epoch 4/10\n",
      "289/289 [==============================] - 6s 22ms/step - loss: 1.0684 - accuracy: 0.4101 - val_loss: 1.0635 - val_accuracy: 0.4382\n",
      "Epoch 5/10\n",
      "289/289 [==============================] - 6s 22ms/step - loss: 1.0596 - accuracy: 0.4266 - val_loss: 1.0650 - val_accuracy: 0.4070\n",
      "Epoch 6/10\n",
      "289/289 [==============================] - 6s 22ms/step - loss: 1.0540 - accuracy: 0.4370 - val_loss: 1.0647 - val_accuracy: 0.4158\n",
      "Epoch 7/10\n",
      "289/289 [==============================] - 6s 22ms/step - loss: 1.0483 - accuracy: 0.4442 - val_loss: 1.0570 - val_accuracy: 0.4372\n",
      "Epoch 8/10\n",
      "289/289 [==============================] - 6s 22ms/step - loss: 1.0420 - accuracy: 0.4503 - val_loss: 1.0591 - val_accuracy: 0.4362\n",
      "Epoch 9/10\n",
      "289/289 [==============================] - 6s 22ms/step - loss: 1.0367 - accuracy: 0.4519 - val_loss: 1.0604 - val_accuracy: 0.4382\n",
      "Epoch 10/10\n",
      "289/289 [==============================] - 7s 23ms/step - loss: 1.0325 - accuracy: 0.4614 - val_loss: 1.0696 - val_accuracy: 0.4362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ff50dbee0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "#model.fit(x_data, y_data, batch_size=32, epochs=10)\n",
    "model.fit(x_data, y_data, validation_split=0.1, batch_size=32, epochs=10, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2409fd5",
   "metadata": {},
   "source": [
    "## Evaluating Precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f1f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/41 [..............................] - ETA: 1s - loss: 1.1487 - accuracy: 0.3438"
     ]
    }
   ],
   "source": [
    "# create the test data\n",
    "tokenizer.fit_on_texts(dataset[\"test\"]['statement'])\n",
    "sequences_statement = tokenizer.texts_to_sequences(dataset[\"test\"]['statement'])\n",
    "\n",
    "tokenizer.fit_on_texts(dataset[\"test\"]['subject'])\n",
    "sequences_subject = tokenizer.texts_to_sequences(dataset[\"test\"]['subject'])\n",
    "\n",
    "#pad the sequences\n",
    "x_test = [pad_sequences(sequences_statement, maxlen=max_words), pad_sequences(sequences_subject, maxlen=max_words)]\n",
    "y_test = to_categorical(test_label, num_classes=output_dim)\n",
    "\n",
    "#evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test, batch_size=32)\n",
    "\n",
    "#print the results\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda0320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
